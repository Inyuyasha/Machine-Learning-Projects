import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('cement_slump.csv')
df.head()

1) Create plots to explore the relationship between each feature and the label.
label = "Compressive Strength (28-day)(Mpa)" # Y-Label
features = ["Cement", "Slag", "Fly ash", "Water", "SP", "Coarse Aggr.", "Fine Aggr.", "SLUMP(cm)", "FLOW(cm)"]
# ^^ X-features.

for feature in features:
    plt.figure(figsize = (8,6))
    sns.scatterplot(x = df[feature], y = df[label], marker = 'o');
    plt.title(f"{feature} vs {label}")
    plt.xlabel(feature)
    plt.ylabel(label)
    plt.show()


2) Train a model using all of the 9 features. You must scale the features.
X = df.drop('Compressive Strength (28-day)(Mpa)', axis = 1)
X.head()

X = df[features] # all 9 X features.
X

y = df['Compressive Strength (28-day)(Mpa)']; #y-label 
y

from sklearn.preprocessing import PolynomialFeatures

poly_converter = PolynomialFeatures (degree = 1, include_bias = False)
poly_features = poly_converter.fit_transform(X)

poly_features.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split (poly_features, y, test_size = 0.3, random_state = 101)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train[:5]
X_train = scaler.transform(X_train)
X_train[:5]
X_test[:5]
X_test = scaler.transform(X_test)
X_test[:5]


2a) Report RMSE of your model and its ratio to the average label value.
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
test_predictions = model.predict(X_test)
from sklearn.metrics import mean_absolute_error, root_mean_squared_error, mean_squared_error
MAE = mean_absolute_error(y_test, test_predictions)
RMSE = root_mean_squared_error(y_test, test_predictions)
MAE
RMSE
RMSE/df['Compressive Strength (28-day)(Mpa)'].mean() # 0.079 error for LinearRegression.
model.coef_


3a) Try different polynomial degrees and identify the optimal degree.
train_rmse_error = []
test_rmse_error = []
for d in range (1,7):
    poly_converter = PolynomialFeatures (degree = d, include_bias = False)
    poly_features = poly_converter.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size = 0.3, random_state = 101)
    scaler = StandardScaler()
    scaler.fit(X_train)
    model = LinearRegression()
    model.fit(X_train, y_train)
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)
    train_RMSE = np.sqrt(mean_squared_error(y_train, train_pred))
    test_RMSE = np.sqrt(mean_squared_error(y_test, test_pred))
    train_rmse_error.append(train_RMSE)
    test_rmse_error.append(test_RMSE)

train_rmse_error
test_rmse_error


plt.plot(range(1,7), train_rmse_error[:6], label = 'Train_RMSE')
plt.plot(range(1,7), test_rmse_error[:6], label = 'Test_RMSE')
plt.xlabel ('Polynomial Degree')
plt.ylabel ('RMSE')
plt.legend();


3b) Create a model using the optimal degree
poly_converter = PolynomialFeatures (degree = 1, include_bias = False)
poly_features = poly_converter.fit_transform(X)

poly_features.shape
X_train, X_test, y_train, y_test = train_test_split (poly_features, y, test_size = 0.3, random_state = 101)
scaler = StandardScaler()
scaler.fit(X_train)
from sklearn.linear_model import LassoCV
lasso_cv_model = LassoCV (eps = 0.001, n_alphas = 100, cv = 5, max_iter=100000)
lasso_cv_model.fit(X_train, y_train)


3b I) Report the RMSE of your model and its ratio to the average label value.
test_predictions = lasso_cv_model.predict(X_test)
MAE = mean_absolute_error (y_test, test_predictions)
RMSE = root_mean_squared_error (y_test, test_predictions)

MAE
RMSE
RMSE/df['Compressive Strength (28-day)(Mpa)'].mean() # 0.058 error using LassoCV model.


3b II) Report the beta coefficients of your model.
lasso_cv_model.coef_


4) Train a Elastic-Net model using ElasticNetCV. You must use scaled polynomial features with the optimal degree identified in 3a.
from sklearn.linear_model import ElasticNetCV


4a) Try 10 different l1_ratio values and 100 alpha values with eps=0.001)
poly_converter = PolynomialFeatures (degree = 1, include_bias = False)
poly_features = poly_converter.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split (poly_features, y, test_size = 0.3, random_state = 101)
scaler = StandardScaler()
scaler.fit(X_train)
elastic_model = ElasticNetCV(l1_ratio = [0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.9, 0.95, 0.99, 1], eps=0.001, n_alphas = 100, cv = 5, max_iter = 100000)
elastic_model.fit(X_train, y_train)
elastic_model.coef_
test_predictions = elastic_model.predict(X_test)
MAE = mean_absolute_error (y_test, test_predictions)
RMSE = root_mean_squared_error (y_test, test_predictions)

4b) Report the RMSE of your model and its ratio to the average label value.
MAE
RMSE
RMSE/df['Compressive Strength (28-day)(Mpa)'].mean() # 0.083 error using LassoCV model.


4c) Report the l1_ratio value and the alpha value chosen by cross-validation
elastic_model.l1_ratio_
elastic_model.alpha_

4d) Report the beta coefficients of your model. Are there any beta coefficients that are zero?
elastic_model.coef_


